{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP Analysis with MitoCheck Single Cells \n",
    "\n",
    "\n",
    "In this notebook, our goal is to apply the Mean Average Precision (MAP) metric, developed in the [copairs](https://github.com/cytomining/copairs) analysis package.\n",
    "We apply this metric to the MitoCheck single-cell dataset, to see the effects of genetic pertubations based on their phenotype. \n",
    "\n",
    "\n",
    "Some links to look at:\n",
    "- copairs [repo](https://github.com/cytomining/copairs)\n",
    "- MitoCheck github [repo](https://github.com/WayScience/mitocheck_data)\n",
    "- MitoCheck zenodo [repo](https://zenodo.org/records/7967386)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import pprint\n",
    "import pathlib\n",
    "\n",
    "from copairs.map import run_pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "# imports src\n",
    "sys.path.append(\"../\")\n",
    "from src import utils\n",
    "\n",
    "# setting up logger\n",
    "# setting up logger\n",
    "logging.basicConfig(filename=\"map_analysis.log\",\n",
    "                    level=logging.DEBUG,\n",
    "                    format='%(levelname)s:%(asctime)s:%(name)s:%(message)s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Downloaded Data\n",
    "\n",
    "In this section, we load the MitoCheck single-cell datasets, including the training, positive controls, and negative controls. \n",
    "For detailed information about the dataset, please refer to the MitoCheck report mentioned above.\n",
    "\n",
    "After downloading the data, we perform formatting by dividing it into two sections. \n",
    "The first section comprises the metadata of each individual cell, while the second section presents all quantified features in a numpy array format.\n",
    "\n",
    "This formatting is designed to easily integrate with the copairs `run_pipeline()` function, allowing for easy execution of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "training_singlecell_data = pathlib.Path(\"../data/raw/training_data.csv.gz\").resolve(strict=True)\n",
    "pos_control_data = pathlib.Path(\"../data/raw/normalized_data/positive_control_data.csv.gz\").resolve(strict=True)\n",
    "neg_control_data = pathlib.Path(\"../data/raw/normalized_data/negative_control_data.csv.gz\").resolve(strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading in the data into dataframe (~10min loading)\n",
    "# training_sc_data = pd.read_csv(training_singlecell_data).drop(\"Unnamed: 0\", axis=1)\n",
    "# pos_control_sc_data = pd.read_csv(pos_control_data)\n",
    "# neg_control_sc_data = pd.read_csv(neg_control_data)\n",
    "\n",
    "# # adding the Mitocheck_Phenotypic_Class into the controls  and labels\n",
    "# pos_control_sc_data.insert(0, \"Mitocheck_Phenotypic_Class\", \"pos_control\")\n",
    "# neg_control_sc_data.insert(0, \"Mitocheck_Phenotypic_Class\", \"neg_control\")\n",
    "\n",
    "# # droping column from trainign data since it does not exist in the controls\n",
    "# training_sc_data = training_sc_data.drop(\"Metadata_Object_Outline\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: delete this later for\n",
    "training_sc_data = pd.read_parquet(\"../data/processed/training_sc_data.parquet\")\n",
    "pos_control_sc_data = pd.read_parquet(\"../data/processed/pos_control_sc_data.parquet\").sample(frac=0.01, random_state=42)\n",
    "neg_control_sc_data = pd.read_parquet(\"../data/processed/neg_control_sc_data.parquet\").sample(frac=0.01, random_state=42)\n",
    "\n",
    "# adding the Mitocheck_Phenotypic_Class into the controls  and labels\n",
    "neg_control_sc_data.insert(0, \"Mitocheck_Phenotypic_Class\", \"neg_control\")\n",
    "\n",
    "# adding control labels into the dataset\n",
    "training_sc_data.insert(1, \"Metadata_is_control\", 0)\n",
    "neg_control_sc_data.insert(1, \"Metadata_is_control\", 1)\n",
    "\n",
    "# droping column from trainign data since it does not exist in the controls\n",
    "training_sc_data = training_sc_data.drop(\"Metadata_Object_Outline\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959027f19a054d6d93c4c21f793cdf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/264476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624ebc4151ce4127be0cf1e84c74fb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3333dda38bb43af9433b7ced5005736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/325 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2542858eb7449b8826199aa02a700c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/264476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32d5351b56f4c8980f703a295f5cffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07782310c804f6593897f8e08079929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/325 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b729c9dcc84cf09708c01189275f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/264476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183cd75f0b50416fb7b1b5afe3e4b669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c1459fb6934053a0c74d4d49e04ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/325 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b151ce0fda42b387950edc0c20b094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/265245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a32894788e4cfea90f348ec0c0cc7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df830bdafde4c89b60847cc3970c54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/451 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4f2b9a871b43feb826b09482c668b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/265245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parameters for pipeline\n",
    "random_state = 42\n",
    "pos_sameby = [\"Mitocheck_Phenotypic_Class\",]\n",
    "pos_diffby = [\"Metadata_Plate\", \"Metadata_Well\"]\n",
    "\n",
    "neg_sameby = [\"Metadata_Plate\"]\n",
    "neg_diffby = [\"Mitocheck_Phenotypic_Class\"]\n",
    "null_size = 100\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "# storing all map results based on postiive and negative controls and feature types\n",
    "map_results_neg_cp = []\n",
    "map_results_neg_dp = []\n",
    "map_results_neg_cp_dp = []\n",
    "\n",
    "# running process\n",
    "# for loop selects one single phenotype\n",
    "# then splits the data into metadata and raw feature values\n",
    "# two different groups that contains 3 splits caused by the types of features\n",
    "# applie the copairs pipeline\n",
    "for phenotype in list(training_sc_data[\"Mitocheck_Phenotypic_Class\"].unique()):\n",
    "\n",
    "    # select training dataset based on phenotype\n",
    "    selected_training = training_sc_data.loc[training_sc_data[\"Mitocheck_Phenotypic_Class\"] == phenotype]\n",
    "\n",
    "    # concatenate to positive and negative control\n",
    "    training_w_neg = pd.concat([selected_training, neg_control_sc_data])\n",
    "\n",
    "    # spliting metadata and raw feature values\n",
    "    logging.info(\"splitting data set into metadata and raw feature values\")\n",
    "    negative_training_cp_meta, negative_training_cp_feats = utils.split_data(training_w_neg, dataset=\"CP\")\n",
    "    negative_training_dp_meta, negative_training_dp_feats = utils.split_data(training_w_neg, dataset=\"DP\")\n",
    "    negative_training_cp_dp_meta, negative_training_cp_dp_feats = utils.split_data(training_w_neg, dataset=\"CP_and_DP\")\n",
    "\n",
    "\n",
    "    # execute pipeline on negative control with trianing dataset with cp features\n",
    "    logging.info(f\"Running pipeline on CP features using {phenotype} phenotype\")\n",
    "    cp_negative_training_result = run_pipeline(meta=negative_training_cp_meta,\n",
    "                                            feats=negative_training_cp_feats,\n",
    "                                            pos_sameby=pos_sameby,\n",
    "                                            pos_diffby=pos_diffby,\n",
    "                                            neg_sameby=neg_sameby,\n",
    "                                            neg_diffby=neg_diffby,\n",
    "                                            batch_size=batch_size,\n",
    "                                            null_size=null_size)\n",
    "\n",
    "    # adding shuffle label column\n",
    "    cp_negative_training_result[\"shuffled\"] = False\n",
    "\n",
    "    # writing out into csv file\n",
    "    cp_negative_training_result.to_csv(f\"{phenotype}_cp_MAP.csv\", index=False)\n",
    "\n",
    "    # append to list\n",
    "    map_results_neg_cp.append(cp_negative_training_result)\n",
    "\n",
    "    # execute pipeline on negative control with trianing dataset with dp features\n",
    "    logging.info(f\"Running pipeline on DP features using {phenotype} phenotype\")\n",
    "    dp_negative_training_result = run_pipeline(meta=negative_training_dp_meta,\n",
    "                                            feats=negative_training_dp_feats,\n",
    "                                            pos_sameby=pos_sameby,\n",
    "                                            pos_diffby=pos_diffby,\n",
    "                                            neg_sameby=neg_sameby,\n",
    "                                            neg_diffby=neg_diffby,\n",
    "                                            batch_size=batch_size,\n",
    "                                            null_size=null_size)\n",
    "\n",
    "    # adding shuffle label column\n",
    "    dp_negative_training_result[\"shuffled\"] = False\n",
    "\n",
    "    # writing out into csv file\n",
    "    dp_negative_training_result.to_csv(f\"{phenotype}_dp_MAP.csv\", index=False)\n",
    "\n",
    "    map_results_neg_dp.append(dp_negative_training_result)\n",
    "\n",
    "    # execute pipeline on negative control with trianing dataset with cp_dp features\n",
    "    logging.info(f\"Running pipeline on CP and DP features using {phenotype} phenotype\")\n",
    "    cp_dp_negative_training_result = run_pipeline(meta=negative_training_cp_dp_meta,\n",
    "                                            feats=negative_training_cp_dp_feats,\n",
    "                                            pos_sameby=pos_sameby,\n",
    "                                            pos_diffby=pos_diffby,\n",
    "                                            neg_sameby=neg_sameby,\n",
    "                                            neg_diffby=neg_diffby,\n",
    "                                            batch_size=batch_size,\n",
    "                                            null_size=null_size)\n",
    "\n",
    "    # adding shuffle label column\n",
    "    cp_dp_negative_training_result[\"shuffled\"] = False\n",
    "\n",
    "    # writing out into csv file\n",
    "    cp_dp_negative_training_result.to_csv(f\"{phenotype}_cp_dp_MAP.csv\", index=False)\n",
    "\n",
    "    map_results_neg_cp_dp.append(cp_dp_negative_training_result)\n",
    "\n",
    "\n",
    "    logging.info(\"Running MAP Pipeline with shuffled data\")\n",
    "    # Below, we are running the same test, but we are shuffling the phenotypes\n",
    "    logging.info(\"Shuffling data based on the Mitocheck_Phenotypic_Class (phenotype) labels\")\n",
    "    shuffled_labels = np.random.permutation(training_w_neg[\"Mitocheck_Phenotypic_Class\"])\n",
    "    training_w_neg[\"Mitocheck_Phenotypic_Class\"] = shuffled_labels\n",
    "\n",
    "    # split the shuffled dataset\n",
    "    # spliting metadata and raw feature values\n",
    "    logging.info(\"splitting shuffled data set into metadata and raw feature values\")\n",
    "    shuffled_negative_training_cp_meta, shuffled_negative_training_cp_feats = utils.split_data(training_w_neg, dataset=\"CP\")\n",
    "    shuffled_negative_training_dp_meta, shuffled_negative_training_dp_feats = utils.split_data(training_w_neg, dataset=\"DP\")\n",
    "    shuffled_negative_training_cp_dp_meta, shuffled_negative_training_cp_dp_feats = utils.split_data(training_w_neg, dataset=\"CP_and_DP\")\n",
    "\n",
    "\n",
    "    # execute pipeline on negative control with trianing dataset with cp features\n",
    "    logging.info(f\"Running pipeline on CP features using {phenotype} phenotype, data is shuffled\")\n",
    "    shuffled_cp_negative_training_result = run_pipeline(meta=shuffled_negative_training_cp_meta,\n",
    "                                            feats=shuffled_negative_training_cp_feats,\n",
    "                                            pos_sameby=pos_sameby,\n",
    "                                            pos_diffby=pos_diffby,\n",
    "                                            neg_sameby=neg_sameby,\n",
    "                                            neg_diffby=neg_diffby,\n",
    "                                            batch_size=batch_size,\n",
    "                                            null_size=null_size)\n",
    "\n",
    "    # adding shuffle label column\n",
    "    shuffled_cp_negative_training_result[\"shuffled\"] = True\n",
    "\n",
    "    # writing out into csv file\n",
    "    shuffled_cp_negative_training_result.to_csv(f\"{phenotype}_cp_shuffled_MAP.csv\", index=False)\n",
    "\n",
    "    # append to list\n",
    "    map_results_neg_cp.append(shuffled_cp_negative_training_result)\n",
    "\n",
    "    # execute pipeline on negative control with trianing dataset with dp features\n",
    "    logging.info(f\"Running pipeline on DP features using {phenotype} phenotype, data is shuffled\")\n",
    "    shuffled_dp_negative_training_result = run_pipeline(meta=shuffled_negative_training_dp_meta,\n",
    "                                            feats=shuffled_negative_training_dp_feats,\n",
    "                                            pos_sameby=pos_sameby,\n",
    "                                            pos_diffby=pos_diffby,\n",
    "                                            neg_sameby=neg_sameby,\n",
    "                                            neg_diffby=neg_diffby,\n",
    "                                            batch_size=batch_size,\n",
    "                                            null_size=null_size)\n",
    "\n",
    "    # adding shuffle label column\n",
    "    shuffled_dp_negative_training_result[\"shuffled\"] = True\n",
    "\n",
    "    # writing out into csv file\n",
    "    shuffled_dp_negative_training_result.to_csv(f\"{phenotype}_dp_shuffled_MAP.csv\", index=False)\n",
    "\n",
    "    map_results_neg_dp.append(shuffled_dp_negative_training_result)\n",
    "\n",
    "    # execute pipeline on negative control with trianing dataset with cp_dp features\n",
    "    logging.info(f\"Running pipeline on CP and DP features using {phenotype} phenotype, data is shuffled\")\n",
    "    shuffled_cp_dp_negative_training_result = run_pipeline(meta=shuffled_negative_training_cp_dp_meta,\n",
    "                                            feats=shuffled_negative_training_cp_dp_feats,\n",
    "                                            pos_sameby=pos_sameby,\n",
    "                                            pos_diffby=pos_diffby,\n",
    "                                            neg_sameby=neg_sameby,\n",
    "                                            neg_diffby=neg_diffby,\n",
    "                                            batch_size=batch_size,\n",
    "                                            null_size=null_size)\n",
    "\n",
    "    # adding shuffle label column\n",
    "    shuffled_cp_dp_negative_training_result[\"shuffled\"] = True\n",
    "\n",
    "    # writing out into csv file\n",
    "    shuffled_cp_dp_negative_training_result.to_csv(f\"{phenotype}_cp_dp_shuffled_MAP.csv\", index=False)\n",
    "\n",
    "    map_results_neg_cp_dp.append(shuffled_cp_dp_negative_training_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitocheck-map-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
